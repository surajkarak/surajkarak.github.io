<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://surajkarak.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://surajkarak.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-05T10:50:42+00:00</updated><id>https://surajkarak.github.io/feed.xml</id><title type="html">Suraj Karakulath</title><subtitle>Suraj Karakulath Data Scientist. </subtitle><entry><title type="html">Exploratory data analysis is much more than just fancy graphs</title><link href="https://surajkarak.github.io/blog/importance-of-eda-data-science/" rel="alternate" type="text/html" title="Exploratory data analysis is much more than just fancy graphs"/><published>2025-08-29T12:22:43+00:00</published><updated>2025-08-29T12:22:43+00:00</updated><id>https://surajkarak.github.io/blog/importance-of-eda-data-science</id><content type="html" xml:base="https://surajkarak.github.io/blog/importance-of-eda-data-science/"><![CDATA[<p>In many of the conversations I have had around data science applications, the focus has often been around modelling, ML algorithms and hyperparameter tuning and deployment into production. Stakeholders often want to know the results - what it means for them or the business overall.</p> <p>But one of the earliest steps in every data science project - that of exploratory data analysis - is severely underrated. This step is often dismissed as simply generating graphs and fancy looking charts, with none of the “meaty” stuff like model training, evaluation or deployment. Even I sometimes feel a bit conscious explaining that this is one aspect of my work in the overall scope of my data science projects.</p> <p>But the more projects I work on, and as I speak to more data scientists, I find the need to emphasise the importance of this step.</p> <h3 id="1-it-is-a-crucial-step-in-understanding-the-data">1. It is a crucial step in understanding the data</h3> <p>I previously wrote about <a href="https://surajkarak.github.io/blog/understanding-data-science-projects/">why it is important to “spend time with data”</a>, before going into subsequent stages of the data science workflow. Taking time to understand the data - how it was extracted, the structure and schema, and what all the features stand for - can save a lot of headache further down the line.</p> <p>EDA becomes an important part of this step. It allows us to understand what features are available, what their distributions are like, how they interact with each other and which features are most influential in any specific target variable you want to predict later on.</p> <h3 id="2-it-can-help-uncover-hidden-data-issues-early">2. It can help uncover hidden data issues early</h3> <p>Sometimes, entire projects are based on flawed data, say mismatched join keys, duplicated time series, inconsistent labels. Think of EDA as your first checkpoint for quality control, before you do any heavy analysis or modelling. Let’s say you are building a sales forecasting model for a retail chain using transactional data. And during EDA, you notice that some stores show negative sales for certain days. This lets you inspect further and maybe you realise that these negative values are due to refunds being logged as negative revenue.</p> <p>Something like this actually happened in one of the projects I worked on with a market-leading energy trading firm in Germany. The task involved finding factors determining the price of wood waste, as the firm’s business involved collecting this waste from sellers and selling it to power plants for energy generation. The data set contained historical values of the price of waste as recorded by the firm, for 3 years.</p> <p>The price values were sometimes negative - these were instances where the firm got paid for collecting the waste from sellers. And when the price values were positive, it was a case of the firm paying money to collect the waste.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/KIPA/data-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/KIPA/data-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/KIPA/data-1400.webp"/> <img src="/assets/img/KIPA/data.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="3-eda-helps-define-your-modelling-strategy">3. EDA helps define your modelling strategy</h3> <p>The insights from EDA often shape your model choice. It helps you figure out the distributions, patterns and relationships in your data, so that you can make more informed decisions about preprocessing steps, feature selection, and model choice. For example, it sheds light on questions like: Are the features linear? Is the target variable balanced? Is it a regression or classification task? The EDA step directs the decision on whether to use a linear model, tree-based method, or even rethink the problem.</p> <p>Again, something I encountered in my work. While working on a churn prediction model for a telecommunication firm, the EDA step revealed that the target variable was heavily imbalanced.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/post_imgs/churn_imbalance-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/post_imgs/churn_imbalance-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/post_imgs/churn_imbalance-1400.webp"/> <img src="/assets/post_imgs/churn_imbalance.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="churn imbalance" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Churn:</p> <p>No: 5174</p> <p>Yes: 1869</p> <p>Almost 73% in favour of one label. This meant that I had to either use SMOTE or class-weighting for handling the imbalance. I also saw that some of the one-hot encoded categorical variables were 100% correlated (redundant) with each other (e.g. InternetService_No and OnlineBackup_No internet service etc.).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/post_imgs/correlation_matrix-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/post_imgs/correlation_matrix-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/post_imgs/correlation_matrix-1400.webp"/> <img src="/assets/post_imgs/correlation_matrix.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="correlation matrix" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This would have led to multicollinearity and one of these pairs of 100% correlated features had to be dropped before modeling.</p> <p>EDA also informed the choice of the model as tree-based models like CatBoost are better at handling non-linearity and categorical variables better than logistic regression.</p> <h3 id="4-you-often-have-to-return-to-eda-multiple-times">4. You often have to return to EDA multiple times</h3> <p>I have often found that conducting EDA is not a one-off task. It is something we have to return to often at a later stage. For example, after modelling, we may find that the error metrics (RMSE, MAE or others) look subpar (or in the other direction - too good to be true). We test out different combinations of features to see how the RMSE changes.</p> <p>This is when we go back to EDA and perhaps plot a correlation matrix of all the features, to find that there are features that are poorly correlated with the target variable, which can be dropped. Or that there are pairs of features that are highly correlated, which leads to multicollinearity - a feature that is undesirable for some ML modelling algorithms.</p> <p>As such, the data science process is not a linear one but rather circular and we have to go back to EDA several times to redo the analysis.</p> <h3 id="5-you-may-have-to-redo-eda-based-on-feedback">5. You may have to redo EDA based on feedback</h3> <p>You may also have to go back to the start and redo the analysis again based on specific feedback you get from stakeholders. This could be due to a gap in understanding context, additional input from stakeholders who are closer to ground truth or changing business priorities.</p> <p>As a simple example, you may find that the business has decided to change their pricing. This means that the features you considered previously for a clustering or MMM optimisation model may not be correct anymore. Or it could be cases where an entire product category has been paused, leading to new null values in your data.</p> <h3 id="6-it-can-be-key-for-stakeholder-communication">6. It can be key for stakeholder communication</h3> <p>I have often found that the insights and visual summaries from the EDA stage can be used to communicate key insights for non-technical stakeholders. Stakeholders don’t always care about F1 scores or neural nets.</p> <p>In presentations to non-technical stakeholders, I have found myself questioning the point of discussing ROC AUC scores and having to go in loops trying to explain confusion matrix and SHAP summary plots. Business teams really care about only how the data supports their decisions, and a lot of technical jargon can be avoided if we reuse the results from the EDA.</p> <p>For example, if you are helping a marketing team understand campaign effectiveness across regions, you may only need to show:</p> <ul> <li>simple box plots of campaign ROI by region and find that one region consistently underperforms.</li> <li>a heatmap of feature correlations shows that age and campaign response are positively correlated — but only in certain income brackets.</li> <li>a simple bar chart showing that a large segment of customers receiving ads had already unsubscribed, indicating poor targeting.</li> </ul> <p>And you could summarise the key insights as something like: “Here’s where we’re overspending and underperforming. If we clean up the targeting and refocus on Region A, we could reduce waste by 25%.”</p> <p>You can keep the results of the machine learning modelling, testing and evaluation to yourself, for future work and validation. Keeping the communication simple can help build trust and connection with the stakeholders - which is crucial, especially when working on long projects.</p> <p><strong>TL;DR</strong>: EDA isn’t a checkbox. It’s the part of data science where you ask the most questions, catch the biggest mistakes, and gain the clearest insights. Ignore it, and your model may work - but not for the right reasons.</p>]]></content><author><name></name></author><category term="data-science"/><category term="data-science"/><summary type="html"><![CDATA[A crucial, underrated step in every data science process.]]></summary></entry><entry><title type="html">Understanding data is half the work</title><link href="https://surajkarak.github.io/blog/understanding-data-science-projects/" rel="alternate" type="text/html" title="Understanding data is half the work"/><published>2025-08-12T17:16:32+00:00</published><updated>2025-08-12T17:16:32+00:00</updated><id>https://surajkarak.github.io/blog/understanding-data-science-projects</id><content type="html" xml:base="https://surajkarak.github.io/blog/understanding-data-science-projects/"><![CDATA[<p>It might sound obvious but in almost all my data science projects, the most crucial step has been to understand the data. The additional value of this step in downstream tasks is self-evident but one cannot overstate how much you stand to lose by ignoring this step.</p> <p>So often, I have made the mistake of taking this step lightly, merely skimming through the schema of dataframes or tables I am working with, thinking to myself “Yeah I get it, let’s move on to the modelling, ML and visualization (meaty stuff)”, only to regret it later. I have almost always had to go back and spend a lot more time understanding the features, their data types and what they stand for in the business context, in order to resolve issues with the downstream tasks.</p> <p>Some simple examples from my work:</p> <h2 id="1-grouping-across-time-ranges">1. Grouping across time ranges</h2> <p>In one project, I had assigned a simple data preprocessing task to a junior data analyst. The task was to aggregate weather data for a number of cities across time and ready for time series analysis. Trusting the work, we proceeded to continue the analysis, only to find later that the data was not grouped per country. It might sound trivial but these are the kinds of errors that can happen if the focus is on getting the code for data transformations without understanding what the data looks like.</p> <h2 id="2-overlapping-week-numbers-at-the-end-of-the-year">2. Overlapping week numbers at the end of the year</h2> <p>Another simple instance was the “overlapping” of week numbers in another time series project. This was my own fault when I converted the date-time values of a time series into week numbers. I realised later that since the last week (span of 7 days in this context) of a year overlapped with day of the new year, which led to some inconsistency in the numbering of weeks. Not always unavoidable but spotting it as early as possible could lead to a lot of time saved in the analysis and modeling.</p> <h2 id="3-understanding-data-protein-data-bank-or-pdb-files-from-a-new-domain">3. Understanding data (Protein Data Bank or PDB files) from a new domain</h2> <p>Yet another example was in my work with the molecular biology research group at my university. In this particular case, I really had to spend a lot of time understanding the data and what it meant as I was not an expert in the domain. At first I tried to power through the code and let the results “show me” what I was doing. But this did not work as I hit a number of roadblocks in the code, and the only solution was to go back to the data, discuss with the research team and understand what I was dealing with in depth.</p> <h2 id="4-merging-adgroup-names-from-2-different-sources">4. Merging adgroup names from 2 different sources</h2> <p>In my work with rebuy, one project required combining names of adgroups from Google Ads performance data and Campaign names from Amplitude, tracked through UTM campaigns. The UTM conventions for tracking was done by a team different from the one that set up the Google ads, which meant that the names were not matching. To address this, I had to sit with the stakeholder and try to discuss with the teams who were responsible for setting up the two different conventions, understand what each represented and why they were set up that way.</p> <p>These experiences reinforced my belief that one should not really take the first step lightly. And that sufficient time must be set aside to dig deep into the data being analysed and explored before going into modelling. If you are working with a client, take the time to sit with the stakeholders to understand the business, context and data.</p>]]></content><author><name></name></author><category term="data-science"/><category term="data-science"/><summary type="html"><![CDATA[It's critical to spend enough time on this first key step in every dat science task]]></summary></entry><entry><title type="html">ChatGPT and the problem with personalisation</title><link href="https://surajkarak.github.io/blog/chatgpt-personalisation-memory/" rel="alternate" type="text/html" title="ChatGPT and the problem with personalisation"/><published>2025-08-08T14:22:23+00:00</published><updated>2025-08-08T14:22:23+00:00</updated><id>https://surajkarak.github.io/blog/chatgpt-personalisation-memory</id><content type="html" xml:base="https://surajkarak.github.io/blog/chatgpt-personalisation-memory/"><![CDATA[<p>There has been much commentary about the issues with ChatGPT and other LLM tools. Hallucination is perhaps the most common. Users are also beginning to notice the excessive agreeability as a problem.</p> <p>But one that I am not seeing talked about enough is that of the level of personalisation. These tools can remember previous conversations and personalise the responses based on this information. The idea is good - to keep responses relevant for you.</p> <p>Sometimes this is necessary and also incredibly useful. I was recently in Paris and not being a French-speaker, ChatGPT was literally my travel buddy. When I got tired and exhausted from walking around the city, I had asked ChatGPT for some recommendations for something to eat that was cheap and healthy. We then had a brief chat about foods with good protein sources and the benefits of omega-3 in reducing inflammation and boosting mood.</p> <p>Later on, I was unsure of what to eat at a restaurant - partly due to the language barrier, and partly due to me indecision. Unable to think clearly, I just took a photo of the menu and asked ChatGPT to help me decide.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/post_imgs/chatgpt-image-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/post_imgs/chatgpt-image-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/post_imgs/chatgpt-image-1400.webp"/> <img src="/assets/post_imgs/chatgpt-image.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="ChatGPT helps me decide lunch" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Bingo, perfect response. This was super helpful and in the end, I did not regret what I ate. Done and done.</p> <p>But having this personalisation always enabled means that new responses may not be ideal when you want answers that are completely independent, and unbiased by context from previous conversations.</p> <p>There is of course a more general “LLM echo chamber” problem where different LLM tools, are trained on data sources that are biased in different ways. These could continue to push users down into their own bubbles, confirming their biases and accelerating polarisation. We have already seen the degradation of social media and networking platforms – starting off as places for information exchange and connecting with diverse groups of people, and ending up becoming fragmented echo chambers where people of similar viewpoints have their biases confirmed.</p> <p>But this kind of personalisation is also something that could actually affect day-to-day usage for practical tasks. I have found responses to new questions being subtly influenced by things I had asked it before.</p> <p>For example, at one point, I was doing a deep dive into media mix modelling and causal inference for a marketing data science project. But later on, when I was brainstorming ideas for fun sideprojects, the response included some suggestions that made use of MMM and causal inference. Not entirely unusable but I was hoping for completely brand new ideas and wondered if the idea suggestions would have been better if I had not discussed these topics separately before.</p> <p>Clearly, this level of personalisation is not always desirable. Sometimes you just want a clean start. I am increasingly having to begin prompts with something like “Forget all other conversations we have had before so as to avoid influencing your answer to this question…” more often, when I need an unbiased response.</p> <p>There is an option to disable this “Memory” in ChatGPT’s Settings but one has to actively do it. I doubt the average user would be aware of this or remember to do it. And disabling it might mean losing the benefits of personalisation when it is actually useful.</p> <p>Like I have always maintained, you have to be really critical of LLM responses and think carefully before taking action based their output.</p>]]></content><author><name></name></author><category term="ai"/><category term="productivity"/><category term="ai"/><category term="chat-gpt"/><summary type="html"><![CDATA[Not every chat needs to be personalised to your interests]]></summary></entry><entry><title type="html">Vibe coding is scary</title><link href="https://surajkarak.github.io/blog/vibe-coding/" rel="alternate" type="text/html" title="Vibe coding is scary"/><published>2025-05-13T10:02:23+00:00</published><updated>2025-05-13T10:02:23+00:00</updated><id>https://surajkarak.github.io/blog/vibe-coding</id><content type="html" xml:base="https://surajkarak.github.io/blog/vibe-coding/"><![CDATA[<p>As with most AI-related trends these days, my first instinct was to treat “vibe coding” as another buzzword created by techbros, purely for marketing and piling on the hype to inflate the bubble.</p> <p>But like ChatGPT, I thought there might be some practical use to some of the tools that were mentioned in the same breath so I decided to check them out. I tried tinkering with <a href="https://www.cursor.com/"><strong>Cursor</strong></a>, <a href="https://github.com/features/copilot"><strong>GitHub</strong></a> Copilot, <a href="https://lovable.dev/"><strong>Lovable</strong></a> and other GenAI tools for a few personal projects. And I landed on a couple of realizations:</p> <h3 id="1-its-scary-good">1. It’s scary good</h3> <p>Scary good how it can get you from idea to mockup in minutes, purely through natural language prompts. This is especially powerful if you use the agent mode to apply code changes directly.</p> <p>I was able to generate a mockup of the basic features I wanted in a web app exactly as I had in mind. And I was not even rigorous in my description of the features, or even in the mental models for how the various features interacted.</p> <p>We’re going to see the time from idea to mockup shrink dramatically. In fact, it’s already happening. Even for experienced developers, coders and data scientists, vibe coding can speed up some of the mundane tasks and focus on the overall logic and workflow. I myself have experienced this in my data science work. The number of times I would forget something as simple as the syntax for filtering rows from a dataframe or joining two tables in an SQL query can be annoying, especially when switching between languages. With vibe coding, I can just avoid the mental load for this and quickly describe in natural language prompt, so I can focus on the downstream tasks like training ML models.</p> <p>Some programmers even take a pseudo-code approach instead of pure natural language prompts and get the correct syntax from the LLM. (”def function(dataframe, feature, number): loop from 1 to number: dataframe[column].average() and return average”)</p> <p>For creatives, designers, makers and entrepreneurs, this opens up so much more power to express themselves. Think of a product designer or SaaS founder who is full of ideas but struggle with coding skills, bandwidth and finding technical co-founders or engineers to implement a prototype before going all in. It’s reminiscent of the “bicycle for the mind” metaphor, except it’s much more than a bicycle.</p> <h3 id="2-its-scary-bad">2. It’s scary bad</h3> <p>Vibe coding can also lead you down an endless loop of misery and frustration if you don’t know what you are doing.</p> <p>In my case, I tried testing how far I could push things after the initial mockup, and kept going: ”Now I want X”, “Add this feature Y too” – still through natural language prompts and still relying on the agent to apply changes without my intervention. I found that this would often result in some correct changes being applied but something else breaking in the process, even things that were already working in the mockup. Now if I had slowed down, added one feature at a time, and asked for more clarity on how to implement each step before proceeding, I’m sure I’d have had better results.</p> <p>This is an ongoing conversation among technology professionals. By nature and in general, we are particular about the details and emphasise rigour and care in the process. But LLM tools tend to give the impression that we can sit back and get to the desired end result without worrying about the details. Which means that when things break in the vibe coding process, it will be necessary to go through individual steps, read every line of code and compare the diffs generated by the AI - which eventually takes more time than a normal process (sans the vibes).</p> <p>So far, my observations with vibecoding are:</p> <ul> <li>If you just want to get from idea to prototype, it’s great - so go full steam ahead</li> <li>For better, more reliable results on more complex projects, you will need embrace friction and go in small increments.</li> <li>Sound prompt engineering practices do make a difference. This means being as clear and detailed as possible with requirements: context, caveats, constraints. Not just in crafting the best prompts to get things done on autopilot but also in knowing when to slow down, take the wheel for bit and prompt with caution.</li> <li>Good ideas will still be valued, as starting points for products, and will be critical in standing out from AI slop</li> <li>Those with technical skills, deep conceptual understanding and experience will still be valued, especially when things break and someone has to step in to fix stuff.</li> </ul>]]></content><author><name></name></author><category term="productivity"/><category term="ai"/><category term="vibe-coding"/><summary type="html"><![CDATA[In a good way and a bad way]]></summary></entry><entry><title type="html">On context switching and some helpful tools</title><link href="https://surajkarak.github.io/blog/context-switching/" rel="alternate" type="text/html" title="On context switching and some helpful tools"/><published>2024-03-29T09:04:16+00:00</published><updated>2024-03-29T09:04:16+00:00</updated><id>https://surajkarak.github.io/blog/context-switching</id><content type="html" xml:base="https://surajkarak.github.io/blog/context-switching/"><![CDATA[<p>Over the last two years and a bit more, I have had to work on more things that were significantly different from each other than before. And I have come to experience the real toll that the mind suffers when context switching.</p> <p>It’s one thing to be absorbed in work all day and feel tired in the evening. But when that work is not of the same “type”, it can be even more mentally draining. Let me illustrate what I mean by this.</p> <p>Let’s say that your daily routine, whether it is a 9 to 5 job, or any kind, involves you doing work in that time period. If you are engaged in the same “kind” of work – coding, designing, writing, speaking whatever – as long as you stick to it, you don’t often have to make a significant mental switch. You will still need to take breaks, rest and get back to what you were doing, but you will still be resuming the same context when you do.</p> <p>But when the type of work you return to is different, or you have to switch between different types of work frequently, that switching can take a slight toll on the mind every time, and it can add up to a lot on top of the usual weariness by the end of the day.</p> <p>For example, in the last two to three years, I have had to spend a lot of time on each of the following tasks at least once, almost every single day:</p> <ul> <li><strong>Work</strong> - where I have had to analyse data, develop ML models and design dashboards, which has involved attending meetings, working on product and code and communicating with various stakeholders over email, Slack and other messaging tools.</li> <li><strong>Reading</strong> - keeping up with the latest developments in tech and AI – LLMs, AI Agents and more</li> <li><strong>Learning</strong> - mainly reinforcing fundamental statistics and math concepts</li> <li><strong>Coursework</strong> - in the form of homework assignments, assessments and thesis for my Master’s programme.</li> <li><strong>Coding</strong> – either for coursework or sideprojects, and it exercises a different part of my brain compared to other types of assignments like researching and reading.</li> <li><strong>Writing</strong> – this includes spending time thinking and putting those thoughts into words</li> <li><strong>Paperwork</strong> - Dealing with German bureaucracy and logistics</li> <li><strong>Language learning</strong> (German) – this started off with Duolingo in 2020, then progressed to in-person classes at my university, reading textbooks and tests myself and finding language partners to practice speaking</li> <li><strong>Sideprojects</strong> – other ideas for coding and data science projects outside of my actual work, for which I had to find inspiration, read up on even more tools and frameworks, come up with ideas and testing and implementing in code</li> </ul> <p>Almost all of these exercise a different muscle in the brain and I found it hard to switch between one to the other.</p> <p>Some simple “mental tools” that I found helpful at some points (I’ll be honest, it’s always a struggle and they don’t work every time) are as follows:</p> <ul> <li><strong>Pomodoro timer:</strong> I set a timer on https://pomofocus.io/ for a session (initially, 25 minutes, then I bumped it to 30 and eventually 1 hour). Not so much for focusing on a task as most people do, but to have that psychological nudge to keep at something. I don’t always stick to finishing a session but it’s some kind of small, atomic, extrinsic motivation to see the time go by as I work no something. And it also helps seeing how much time I have spent in total at the end of the day on various tasks.</li> <li><strong>Telling myself that I only need to finish a small, tiny step</strong>: So often, I have tried to get started on a new task after switching from another one of a different context, and got stuck trying to find the dopamine boost. The slight bit of unfamiliarity in the new task, or memories of being stuck on the last problem in that task or other frictions or the effort to try and recall where exactly I was have led to me spending minutes trying to force myself to focus but failing. In times like these, sometimes I have managed to overcome this block by telling myself that I only need to go to the last line of the code or write-up and make one minor incremental progress. As I do this, my focus slowly begins to grow and I end up spending more time and making more progress.</li> <li><strong>Power naps:</strong> I have found immense value in taking a power nap in the middle of the day. I realised that my lack of motivation and focus started around the middle of the day, after I had spent a lot of time thinking in the morning. By around noon my brain is often cluttered with various thoughts, confusing ideas and distractions. A power nap helps cleanse it and declutter the cobwebs so to speak, almost deleting memories, concerns and irrelevant thoughts, or a kind of factory reset.</li> <li><strong>Long walks/runs</strong>: At the end of the day, I am usually grumpy and exhausted but only mentally. Physically though, I am usually craving for some movement. I have found that just getting out and going for walks (during which I also do short sprints) helps me clear my mind a little. I am not always trying to do any cleansing but the opposite – I just let my mind wander off wherever it wants to go. I don’t practice mindfulness or meditation but I have heard some people talk about “walking meditation” and I have a feeling this might be what I am doing, without realising it. Anyway, once I return from the walk, I am usually feeling much better, my mind is a little cleared up and the well-known endorphin high is also working its thing – all of which together make it easier to get back into doing something productive.</li> </ul>]]></content><author><name></name></author><category term="productivity"/><category term="context-switching"/><summary type="html"><![CDATA[How I try to deal with this silent productivity killer]]></summary></entry></feed>